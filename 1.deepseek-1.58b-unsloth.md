# 使用unsloth在MAC或小显卡/无显卡ubuntu机器上部署deepseek 1.58b量化模型（满血版，非蒸馏版）

## 特点

不要求显存，内存（20GB）和磁盘IO要求高，因为GPU offload出来的稀疏矩阵计算量都给了CPU，所以内存使用量较大，磁盘IO较大是因为激活不同的专家模型时，需要从磁盘中读取。

## 参考手册

https://unsloth.ai/blog/deepseekr1-dynamic

## 命令

```shell
# 因为模型有131GB，建议先在后台开始下载模型，再同时准备其他步骤；参考https://hf-mirror.com/，选择喜欢的拉镜像工具
sudo apt-get update
sudo apt-get install aria2 build-essential cmake curl libcurl4-openssl-dev -y
wget https://hf-mirror.com/hfd/hfd.sh
chmod a+x hfd.sh
export HF_ENDPOINT=https://hf-mirror.com
GIT_CLONE_PROTECTION_ACTIVE=false nohup ./hfd.sh unsloth/DeepSeek-R1-GGUF --include *UD-IQ1_S* --tool aria2c -x 4 2>&1 &

# 下载llama.cpp项目源码，用于运行和转换模型
git clone https://github.com/ggerganov/llama.cpp
# 注意以下编译选项-DGGML_CUDA=ON，是在有NVIDIA显卡时候才能用；如果是MacOS，改成cmake llama.cpp -B llama.cpp/build；如果只有CPU，要改成-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS；其他情况请参考https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md
cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON
# 也可以使用cmake --build . --config Release替换以下命令
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split
cp llama.cpp/build/bin/llama-* llama.cpp

# 用编译好的llama.cpp运行模型，你需要根据实际情况修改以下参数：
# --model 实际模型下载路径下的DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf文件
# --threads 物理核数，由命令 cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l 获得
# --ctx-size 你预计模型输出的文本最大长度
# --seed 随机数，文章最上方提到他们用的是3407、3408、3409
# --prompt 你的问题，我的样例命令中只有“你好”这两个字能替换
# --n-gpu-layers 加载的模型层数，这个太大会引起OOM，所以要么用文章中提到的公式计算，要么按照下方的表格，根据自己实际情况测试下最大可用数值
./llama.cpp/llama-cli --model /data/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf --cache-type-k q4_0 --threads 32 --prio 2 --temp 0.6 --ctx-size 2048 --seed 3419 --n-gpu-layers 35 -no-cnv --prompt "<｜User｜>你好<｜Assistant｜>"

# 将3个GGUF模型文件合并为一个
./llama.cpp/llama-gguf-split --merge /data/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf merged_file.gguf

# 编写Modelfile，用ollama加载模型；以下num_gpu与上面llama-cli的--n-gpu-layers参数值一致
cat Modelfile
FROM ./merged_file.gguf
TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1}}
{{- if eq .Role "user" }}<｜User｜>{{ .Content }}
{{- else if eq .Role "assistant" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}
{{- end }}
{{- if and $last (ne .Role "assistant") }}<｜Assistant｜>{{- end }}
{{- end }}"""
PARAMETER stop <｜begin▁of▁sentence｜>
PARAMETER stop <｜end▁of▁sentence｜>
PARAMETER stop <｜User｜>
PARAMETER stop <｜Assistant｜>
PARAMETER use_mmap true
PARAMETER num_gpu 35

ollama create deepseek-r1:671b
ollama list
ollama run deepseek-r1:671b
```
